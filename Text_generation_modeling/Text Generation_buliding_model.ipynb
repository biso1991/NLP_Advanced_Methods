{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation of a character-level LSTM (Long Short-Term Memory) model using TensorFlow and Keras. \n",
    "# this model is trained on the text from \"Alice's Adventures in Wonderland\" and is then used to generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "huCNBa-AIBUs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 15:25:27.440633: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-24 15:25:28.125364: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 15:25:28.125475: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 15:25:28.129328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 15:25:28.476004: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-24 15:25:28.479394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 15:25:31.265943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install tensorflow==2.0.0 numpy requests tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIBo0gh4IFUS",
    "outputId": "6c1fdbaa-2752-4d64-e120-58f2c705e428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167711"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wNl5ZHPUII87"
   },
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "# dataset file path\n",
    "FILE_PATH = \"wonderland.txt\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "# read the data\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
    "# remove caps, comment this code if you want uppercase characters as well\n",
    "text = text.lower()\n",
    "# remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej-SscMwILbm",
    "outputId": "132ef5cd-06f0-4681-b450-0439def4b3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: \n",
      " 0123456789abcdefghijklmnopqrstuvwxyzù—‘’“”•™﻿\n",
      "Number of characters: 158225\n",
      "Number of unique characters: 47\n"
     ]
    }
   ],
   "source": [
    "# print some stats\n",
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text)))\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6EtXDd_sINa1"
   },
   "outputs": [],
   "source": [
    "# dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "# dictionary that converts integers to characters\n",
    "int2char = {i: c for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K8V6LzelIPTt"
   },
   "outputs": [],
   "source": [
    "# save these dictionaries for later generation\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_dH9ZMDzIQ5U"
   },
   "outputs": [],
   "source": [
    "# convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_u-lnbxBI5Sj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 15:27:06.426859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 15:27:06.481282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw5fbVfTI7-L",
    "outputId": "a613fce4-2bd8-48e6-e310-124f747aa894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 ﻿\n",
      "31 t\n",
      "19 h\n",
      "16 e\n",
      "1  \n",
      "27 p\n",
      "29 r\n",
      "26 o\n"
     ]
    }
   ],
   "source": [
    "# print first 5 characters\n",
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeOe7V3gI9MT",
    "outputId": "20c7fbec-711a-413b-b06c-9814a6ca8e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of alices adventures in wonderland\n",
      "    \n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "most other parts of the world at no cost and with almost no re\n",
      "strictions\n",
      "whatsoever you may copy it give it away or reuse it under the terms\n",
      "of the project gutenberg license included with this ebook or online\n",
      "at wwwgutenbergorg if you are not located in the unite\n"
     ]
    }
   ],
   "source": [
    "# build sequences by batching\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ri5Z4GCmI_CR"
   },
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2):\n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "        # extend the dataset with these samples by concatenate() method\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "# prepare inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VreHeB5mJDK0"
   },
   "outputs": [],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # onehot encode the inputs and the targets\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkoAHekjJE7T",
    "outputId": "dfa9562d-69ae-4e78-b55a-8ceb58481221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ﻿the project gutenberg ebook of alices adventures in wonderland\n",
      "    \n",
      "this ebook is for the use of an\n",
      "Target: y\n",
      "Input shape: (100, 47)\n",
      "Target shape: (47,)\n",
      "================================================== \n",
      "\n",
      "Input: the project gutenberg ebook of alices adventures in wonderland\n",
      "    \n",
      "this ebook is for the use of any\n",
      "Target: o\n",
      "Input shape: (100, 47)\n",
      "Target shape: (47,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 2 samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rgWLRKLhJGfC"
   },
   "outputs": [],
   "source": [
    "# repeat, shuffle and batch the dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4YMvYwpyJKHr"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYgORUFeJLsc",
    "outputId": "d06e1dbf-bac1-42bc-fd87-1a82109daa30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235/1235 [==============================] - 1335s 1s/step - loss: 2.3181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bilel/anaconda3/envs/base2/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# make results folder if does not exist yet\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "# train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=1)\n",
    "# save the model\n",
    "model.save(f\"results/{BASENAME}-{sequence_length}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "O564yOs5JlH8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
    "import os\n",
    "\n",
    "sequence_length = 100\n",
    "# dataset file path\n",
    "FILE_PATH = \"wonderland.txt\"\n",
    "# FILE_PATH = \"data/python_code.py\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "seed = \"chapter xiii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4GHGMhvBKkcR"
   },
   "outputs": [],
   "source": [
    "# load vocab dictionaries\n",
    "char2int = pickle.load(open(f\"{BASENAME}-char2int.pickle\", \"rb\"))\n",
    "int2char = pickle.load(open(f\"{BASENAME}-int2char.pickle\", \"rb\"))\n",
    "vocab_size = len(char2int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "n0XeGzsxKr_n"
   },
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M8dQkpj8KuCL"
   },
   "outputs": [],
   "source": [
    "# load the optimal weights\n",
    "model.load_weights(f\"results/{BASENAME}-{sequence_length}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVI7mpYoK-Py",
    "outputId": "071df73f-c771-4eb6-9a27-684617385aef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:   0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 400/400 [00:38<00:00, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: chapter xiii\n",
      "Generated text:\n",
      "n a dound and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg and the project gutenberg \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = seed\n",
    "n_chars = 400\n",
    "# generate 400 characters\n",
    "generated = \"\"\n",
    "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
    "    # make the input sequence\n",
    "    X = np.zeros((1, sequence_length, vocab_size))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "    # converting the integer to a character\n",
    "    next_char = int2char[next_index]\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", s)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q25hcgohK_eB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5.9.1 Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
